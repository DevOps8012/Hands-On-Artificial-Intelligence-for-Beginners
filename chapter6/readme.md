# Chapter 6: Recurrent Neural Networks

Recurrent neural networks (RNN) are the most flexible form of networks and are widely used in natural language processing (NLP), financial services, and a variety of other fields. Vanilla feedforward networks, as well as their convolutional varieties, accept a fixed input vector and output a fixed vector; they assume that all of your input data is independent of each other. RNNs, on the other hand, operate on sequences of vectors and output sequences of vectors, and allow us to handle many exciting types of data. RNNs are actually turing-complete, in that they can simulate arbitrary tasks, and hence are very appealing models from the perspective of the artificial intelligence scientist. 

## Code

- *Chapter6.ipynb*: Follows along with the code in the chapter

## Datasets

The primary dataset can be downloaded from the University of California, Berkley's ML Group [here](https://drive.google.com/file/d/0B5o40yxdA9PqTnJuWGVkcFlqcG8/view)

image_vectors.token contains the corresponding captions for the dataset. 


