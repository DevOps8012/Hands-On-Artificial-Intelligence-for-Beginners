{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Platforms and Other Essentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This chapter discusses the important libraries and frameworks that one needs to get started in artificial intelligence. We'll cover the basic functions of the three most popular deep learning frameworks: Tensorflow, Pytorch, and Keras, and show you how to get up and running in each of these frameworks as we will be utilizing them in the following chapters. We'll touch upon computing for Artificial Intelligence, and discuss how GPUs and other advanced memory units can improve AI. Lastly, we'll discuss the fundamentals of two popular cloud computing frameworks for deep learning, AWS and Google Cloud. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Tensorflow at tf for simplicity\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define two constants \n",
    "const1 = tf.constant([])\n",
    "const2 = tf.constant([])\n",
    "\n",
    "# Multiply the constants\n",
    "product = tf.multiply(x1, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In Tensorflow, we use sessions to run blocks of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\n"
     ]
    }
   ],
   "source": [
    "# Import Tensorflow at tf for simplicity\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define two constants \n",
    "const1 = tf.constant([4])\n",
    "const2 = tf.constant([5])\n",
    "\n",
    "# Multiply the constants\n",
    "product = tf.multiply(const1, const2)\n",
    "\n",
    "# In Tensorflow, we must first initialize a session object\n",
    "sess = tf.Session()\n",
    "\n",
    "# Run the session\n",
    "print(sess.run(product)) \n",
    "\n",
    "# Close the session\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As Keras is designed as model level library, it does not contain methods of doing basic operations as PyTorch of base TensorFlow does, instead, it utilizes either a Tensorflow or Theano (another library) as a backend. As such, it's basic operations to not vary much from base TensorFlow operations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "varA = K.constant(5)\n",
    "varB = K.constant(6)\n",
    "\n",
    "product = varA * varB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "varA = torch.IntTensor([4])\n",
    "varB = torch.IntTensor([5])\n",
    "\n",
    "product = varA * varB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As a result of it's native Python feel, PyTorch allows for easy interaction between standard numpy arrays and PyTorch tensors. It's easy to switch back and forth between the two: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "\n",
    "# Create a numpy array\n",
    "numpy_array = np.random.randn(20,20)\n",
    "\n",
    "# Convert the numpy array to a pytorch tesnor\n",
    "pytorch_tensor = torch.from_numpy(numpy_array)\n",
    "\n",
    "# Back to Numpy\n",
    "numpy_again = pytorch_tensor.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch Tensors can easily be indexed and sliced in a pythonic way as well:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_var = torch.FloatTensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "print(x[1][2])\n",
    "6.0\n",
    "\n",
    "x[0][1] = 8\n",
    "print(x)\n",
    " 1 8 3\n",
    " 4 5 6\n",
    "[torch.FloatTensor of size 2x3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Your GPU for Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You can check for NVIDIA GPU with the command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "lspci -nnk | grep -i nvidia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up your GPU with Linux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started with deep learning for GPUs on Ubuntu, we first have to install the GPU's driver. In this example, we're going to utilize wget and chmod to retrieve and setup read/write access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wget http://us.download.nvidia.com/XFree86/Linuxx86_64/367.44/NVIDIA-Linux-x86_64-367.44.run\n",
    "\n",
    "sudo chmod +x NVIDIA-Linux-x86_64-367.35.run\n",
    "./NVIDIA-Linux-x86_64-367.35.run --silent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the installation finishes, you can check if it was intalled correctly with a simple nvidia-smi command.\n",
    "Next, let's install NVIDIA CUDA. CUDA is an NVIDIA package that allows us to run Tensorflow models on our GPUs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wget\"http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_8.0.44-1_amd64.deb\"\n",
    "\n",
    "## Install the drivers\n",
    "sudo chmod +x cuda_7.5.18_linux.run\n",
    "./cuda_7.5.18_linux.run --driver --silent\n",
    "./cuda_7.5.18_linux.run --toolkit --silent\n",
    "./cuda_7.5.18_linux.run --samples --silent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's add the library to our system path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "echo ‘export LD_LIBRARY_PATH=”$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\"’ >> ~/.bashrc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need to install a higher level package called cuNN, which is a specific library that sits on top of CUDA and provides highly tuned procedures for typical deep learning operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sudo scp cudnn-7.0-linux-x64-v4.0-prod.tgz \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last step to move the files to the correct place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tar -xzvf cudnn-7.0-linux-x64-v4.0-prod.tgz\n",
    "cp cuda/lib64/* /usr/local/cuda/lib64/\n",
    "cp cuda/include/cudnn.h /usr/local/cuda/include/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ...and voilà, we're setup on Ubuntu for GPU acceleration. Our last step is simply to install the GPU enabled version of TensorFlow with Python 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pip3 install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic GPU Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates a graph.\n",
    "c = []\n",
    "for d in ['/gpu:0', '/gpu:1']:\n",
    "    with tf.device(d):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3])\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],shape [3,2]) \n",
    "    c.append(tf.matmul(a, b))\n",
    "with tf.device('/cpu:0'):\n",
    "    sum = tf.add_n(c)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# Runs the op.\n",
    "print sess.run(sum)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
