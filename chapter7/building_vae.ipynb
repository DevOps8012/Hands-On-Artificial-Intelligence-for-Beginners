{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: Building a Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative models are the most promising push toward enabling computers to have an understanding of the world. They are true unsupervised models, and are able to perform those tasks that many today consider to be at the cutting edge of Artificial Intelligence (AI). Generative models are different for precisely the reason as it sounds: they generate data. Centered mostly around computer vision tasks, this class of network has the power to create new faces, new handwriting, or even paintings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Variational AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoder(x, initializer):\n",
    "    \n",
    "    input_layer = tf.layers.dense(inputs=x, units=784, activation=tf.nn.elu,\n",
    "                                 kernel_initializer=initializer, bias_initializer=initializer,\n",
    "                                 name='input_layer'\n",
    "                                 )\n",
    "    \n",
    "    hidden_1 = tf.layers.dense(inputs=input_layer, units=256, activation=tf.nn.elu,\n",
    "                             kernel_initializer=initializer, bias_initializer=initializer\n",
    "                             )\n",
    "    \n",
    "    hidden_2 = tf.layers.dense(inputs=hidden_1, units=128, activation=tf.nn.elu,\n",
    "                       kernel_initializer=initializer, bias_initializer=initializer\n",
    "                              )\n",
    "    \n",
    "    ## Calculate mu and sigma for the z distribtion\n",
    "    mu = tf.layers.dense(inputs=hidden_2, units=10, activation=None)\n",
    "    sigma = tf.layers.dense(inputs=hidden_2, units=10, activation=None)\n",
    "   \n",
    "    ## Calculate z\n",
    "    epsilon = tf.random_normal(shape=tf.shape(sigma), mean=0, stddev=1, dtype=tf.float32)\n",
    "    z = mu + tf.sqrt(tf.exp(sigma)) * epsilon\n",
    "    \n",
    "    ## Calculate the KL Divergence\n",
    "    kl_div = -0.5 * tf.reduce_sum( 1 + sigma - tf.square(mu) - tf.exp(sigma), axis=1)\n",
    "    kl_div = tf.reduce_mean(latent_loss)\n",
    "    \n",
    "    \n",
    "    return z, kl_div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoder(z, initializer):\n",
    "    layer_1 = fully_connected(z, 256, scope='dec_l1', activation_fn=tf.nn.elu, \n",
    "                             kernel_initializer=initializer, bias_initializer=initializer\n",
    "                             )\n",
    "    layer_2 = fully_connected(layer_1, 384, scope='dec_l2', activation_fn=tf.nn.elu,\n",
    "                             kernel_initializer=initializer, bias_initializer=initializer\n",
    "                             )\n",
    "    layer_3 = fully_connected(layer_2, 512, scope='dec_l3', activation_fn=tf.nn.elu,\n",
    "                             kernel_initializer=initializer, bias_initializer=initializer\n",
    "                             )\n",
    "    dec_out = fully_connected(layer_3, input_dim, scope='dec_l4', activation_fn=tf.sigmoid,\n",
    "                             kernel_initializer=initializer, bias_initializer=initializer\n",
    "                             )\n",
    "    \n",
    "    ## Calculate the reconstruction loss\n",
    "    epsilon = 1e-10\n",
    "    rec_loss = -tf.reduce_sum(x * tf.log(epsilon + dec_out) + (1 - x) * tf.log(epsilon + 1 - dec_out), axis=1)\n",
    "    rec_loss = tf.reduce_mean(rec_loss)\n",
    "    \n",
    "    return dec_out, rec_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "batch_size = 100\n",
    "n_z = 10\n",
    "epochs = 100\n",
    "input_dim = 784 \n",
    "num_sample = 55000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the Encoder Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(name='x', dtype='float', shape=[None, input_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the model, loss, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## initialize the models\n",
    "z, kl_div = encoder(x)\n",
    "dec_out, rec_loss = decoder(x)\n",
    "\n",
    "## Calculate the overall model loss term\n",
    "loss = tf.reduce_mean(rec_loss + kl_div)\n",
    "\n",
    "## Create the optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "## Create the weight initializer\n",
    "initializer = tf.contrib.layers.xavier_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for iter in range(num_sample // batch_size):\n",
    " \n",
    "            batch_x = mnist.train.next_batch(batch_size)\n",
    "    \n",
    "            _, l, rl, ll = sess.run([optimizer, loss, rec_loss, kl_div], feed_dict={x: batch_x[0]})\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print('[Epoch {}] Total Loss: {}, Reconstruction Loss: {}, Latent Loss: {}'.format(epoch, l, rl, ll))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate New Samples\n",
    "Code from @shaohua0116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = np.random.normal(size=[batch_size, n_z])\n",
    "x_generated = x_hat = self.sess.run(dec_out, feed_dict={z: z})\n",
    "\n",
    "n = np.sqrt(batch_size).astype(np.int32)\n",
    "I_generated = np.empty((h*n, w*n))\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        I_generated[i*h:(i+1)*h, j*w:(j+1)*w] = x_generated[i*n+j, :].reshape(28, 28)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(I_generated, cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
