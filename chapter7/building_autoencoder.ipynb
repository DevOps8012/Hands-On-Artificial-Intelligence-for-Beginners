{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: Building and AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative models are the most promising push toward enabling computers to have an understanding of the world. They are true unsupervised models, and are able to perform those tasks that many today consider to be at the cutting edge of Artificial Intelligence (AI). Generative models are different for precisely the reason as it sounds: they generate data. Centered mostly around computer vision tasks, this class of network has the power to create new faces, new handwriting, or even paintings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patricksmith/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the weight and bias initializer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initializer = tf.contrib.layers.xavier_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoder(x):\n",
    "    input_layer = tf.layers.dense(inputs=x, units=784, activation=tf.nn.relu,\n",
    "                                 kernel_initializer=initializer, bias_initializer=initializer \n",
    "                                 )\n",
    "    z_prime = tf.layers.dense(inputs=input_layer, units=256, activation=tf.nn.relu,\n",
    "                             kernel_initializer=initializer, bias_initializer=initializer\n",
    "                             )\n",
    "    z = tf.layers.dense(inputs=z_prime, units=128, activation=tf.nn.relu,\n",
    "                       kernel_initializer=initializer, bias_initializer=initializer\n",
    "                       )\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoder(x):\n",
    "    x_prime_one = tf.layers.dense(inputs=x, units=128, activation=tf.nn.relu,\n",
    "                                 kernel_initializer=initializer, bias_initializer=initializer\n",
    "                                 )\n",
    "    x_prime_two = tf.layers.dense(inputs=x_prime_one, units=256, activation=tf.nn.relu,\n",
    "                                 kernel_initializer=initializer, bias_initializer=initializer\n",
    "                                 )\n",
    "    output_layer = tf.layers.dense(inputs=x_prime_two, units=784, activation=tf.nn.relu,\n",
    "                                  kernel_initializer=initializer, bias_initializer=initializer\n",
    "                                  )\n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display = 1\n",
    "input_dim = 784 \n",
    "learning_rate = 0.001\n",
    "num_steps = 30000\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", [None, input_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct the encoder, decoder, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the full autoencoder\n",
    "z = encoder(x)\n",
    "\n",
    "## x_prime represents our predicted distribution\n",
    "x_prime = decoder(z) \n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "loss = tf.reduce_mean(tf.pow(x - x_prime, 2))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the training cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Begin a new tensorflow session:\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    ## Training Loop\n",
    "    for i in range(1, num_steps+1):\n",
    "    \n",
    "        ## Feed Batches of MNIST Data\n",
    "        batch_x, _ = mnist.train.next_batch(batch_size)\n",
    "\n",
    "        ## Run the Optimization Process\n",
    "        _, l = sess.run([optimizer, loss], feed_dict={x: batch_x})\n",
    "\n",
    "        ## Display the loss at every 1000 out of 30,000 steps\n",
    "        if i % display == 0 or i == 1:\n",
    "            print('Step %i: Loss: %f' % (i, l))\n",
    "\n",
    "    n = 4\n",
    "    canvas_orig = np.empty((28 * n, 28 * n))\n",
    "    canvas_recon = np.empty((28 * n, 28 * n))\n",
    "\n",
    "    for i in range(n):\n",
    "\n",
    "        batch_x, _ = mnist.test.next_batch(n)\n",
    "\n",
    "        # Encode and decode each individual written digit\n",
    "        g = sess.run(decoder, feed_dict={x: batch_x})\n",
    "\n",
    "        # Display original images\n",
    "        for j in range(n):\n",
    "\n",
    "            # Draw the original digits\n",
    "            canvas_orig[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = batch_x[j].reshape([28, 28])\n",
    "\n",
    "        # Display reconstructed images\n",
    "        for j in range(n):\n",
    "\n",
    "            # Draw the reconstructed digits\n",
    "            canvas_recon[i * 28:(i + 1) * 28, j * 28:(j + 1) *                                        28] = g[j].reshape([28, 28])\n",
    "\n",
    "    # Plot the original image vs the reconstructed images. \n",
    "    print(\"Original Images\")\n",
    "    plt.figure(figsize=(n, n))\n",
    "    plt.imshow(canvas_orig, origin=\"upper\", cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Reconstructed Images\")\n",
    "    plt.figure(figsize=(n, n))\n",
    "    plt.imshow(canvas_recon, origin=\"upper\", cmap=\"gray\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
