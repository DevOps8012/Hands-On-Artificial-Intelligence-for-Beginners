{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8: Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along with generative networks, reinforcement learning algorithms provide the most visible advances in artificial intelligence today. For many years, computer scientists have worked toward creating algorithms and machines that can perceive and react to their environment like a human would. Reinforcement learning is a manifestation of that, giving us the wildly popular AlphaGo and self-driving cars. In this chapter, we'll cover the foundations of reinforcement learning that will allow us to create advanced artificial agents later in this book. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Cartpole with a Random Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a Random Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_policy():\n",
    "    return np.random.uniform(-1,1, size=4), np.random.uniform(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Translate that policy into action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def action(env, policy, obs):\n",
    "    if np.dot(policy[0], obs) + policy[1] > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate the Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the Random Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Generate a list of policies and their potential scores\n",
    "policy_list = [generate_policy() for _ in range(1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a function to Run a Training Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_train_episode(env, p):\n",
    "    obs = env.reset()\n",
    "    ep_reward = 0\n",
    "    for i in range(1000):\n",
    "        env.render()\n",
    "        selected_action = action(env, policy, obs)\n",
    "        obs, reward, done, _ = env.step(selected_action)\n",
    "        ep_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Run an Episode Using the Random Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policy = None\n",
    "total_reward = 0\n",
    "for i in range(1000):\n",
    "    policy = generate_policy()\n",
    "    episode_reward = run_train_episode(env, policy)\n",
    "    if episode_reward > total_reward:\n",
    "        total_reward = reward\n",
    "        best_policy = policy\n",
    "        if total_reward == 1000:\n",
    "            break\n",
    "\n",
    "print('Optimal Policy Is: = %f, Total Reward Is: %f' %optimal_policy %total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an Agent With Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_buckets = (1,1,6,3)\n",
    "episodes = 200\n",
    "episode_length = 250\n",
    "number_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Q Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros(num_buckets + (number_actions,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Epsilon Greedy Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy(episode):\n",
    "    return max(0.01, min(1, 1.0 - math.log10((episode+1)/25)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaptive Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learning_rate(episode):\n",
    "    return max(0.1, min(0.5, 1.0 - math.log10((episode+1)/25)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Have the agent choosing an action based on the epsilon greedy strategy; explore or exploit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_action(state, episode):\n",
    "    if random.uniform(0,1) < epsilon_greedy(episode):\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = np.argmax(q_table[state,:])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bellman Equation to Update the Q Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bellman(current_state, new_state, action, reward, episode):\n",
    "    best_q = np.amax(q_table[new_state])\n",
    "    q_table[current_state + (action,)] += get_learning_rate(episode)*(reward + 0.99*(best_q) - q_table[current_state + (action,)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equation to handle storing state information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bucket(state, bucket_len_arr):\n",
    "    bucket_indice = []\n",
    "    bounds = list(zip(env.observation_space.low, env.observation_space.high))\n",
    "    bounds[1] = [-0.5, 0.5]\n",
    "    bounds[3] = [-math.radians(50), math.radians(50)]\n",
    "    \n",
    "    \n",
    "    for i in range(len(state)):\n",
    "        if state[i] <= bounds[i][0]:\n",
    "            bucket_index = 0\n",
    "        elif state[i] >= bounds[i][1]:\n",
    "            bucket_index = bucket_len_arr[i] - 1\n",
    "        else:\n",
    "            bound_width = bounds[i][1] - bounds[i][0]\n",
    "            offset = (bucket_len_arr[i]-1)*bounds[i][0]/bound_width\n",
    "            scaling = (bucket_len_arr[i]-1)/bound_width\n",
    "            bucket_index = int(round(scaling*state[i] - offset))\n",
    "        bucket_indice.append(bucket_index)    \n",
    "    return tuple(bucket_indice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward = []\n",
    "for episode in range(episodes):\n",
    "    current_state = bucket(env.reset(), (1,1,6,3))\n",
    "    \n",
    "    ## Initialize the reward\n",
    "    episode_reward = 0\n",
    "    for leng in range(episode_length):\n",
    "        env.render()\n",
    "        action = choose_action(current_state, episode)\n",
    "        obv, reward, done, _ = env.step(action)\n",
    "        new_state = bucket(obv, (1,1,6,3))\n",
    "        \n",
    "        ## Update the q-table using the bellman equation\n",
    "        \n",
    "        print(new_state)\n",
    "        bellman(current_state, new_state, action, reward, episode)\n",
    "        \n",
    "        ## Update the current state to the new selected state\n",
    "        current_state = new_state\n",
    "        \n",
    "        ## Set the current reward as the episode reward\n",
    "        episode_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "total_reward.append(episode_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
